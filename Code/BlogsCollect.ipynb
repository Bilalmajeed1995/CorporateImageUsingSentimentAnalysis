{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from newspaper import Article\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from newspaper.article import ArticleException, ArticleDownloadState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_lst = [\"apple\", \"microsoft\", \"gamestop\", \"mercedes-benz\", \"tesla\",\n",
    "               \"toyota\", \"nike\", \"adidas\", \"walmart\", \"target\", \"the-home-depot\",\n",
    "               \"best-buy\", \"costco\", \"hyatt-hotels\", \"marriott-hotels-resorts\", \"verizon\", \n",
    "               \"att\", \"citigroup\", \"jpmorgan-chase\", \"wells-fargo\", \"mcdonalds\",\n",
    "               \"berkshire-hathaway\", \"delta-air-lines\", \"state-farm\", \"starbucks\",\n",
    "               \"allstate-insurance\", \"cvs-health\", \"rogers-communications\", \"cineplex\"]\n",
    "check_lst = [\"Apple\", \"Microsoft\", \"GameStop\", \"Mercedes\", \"Tesla\",\n",
    "              \"Toyota\", \"Nike\", \"Adidas\", \"Walmart\", \"Target\", \"Home Depot\",\n",
    "              \"Best Buy\", \"Costco\", \"Hyatt\", \"Mariott\", \"Verizon\", \n",
    "              \"AT&T\", \"Citi\", \"JPMorgan\", \"Wells\", \"McDonald\", \"Berkshire\", \"Delta\", \n",
    "              \"State Farm\", \"Starbucks\", \"Allstate\", \"CVS\", \"Rogers\", \"Cineplex\"]\n",
    "links = []\n",
    "\n",
    "for idx, search in enumerate(search_lst):\n",
    "    for page_number in range(1,15): \n",
    "        page_number=str(page_number)\n",
    "        \n",
    "        if search == 'cineplex' or search == 'rogers-communications':\n",
    "            base_url_huff = 'https://www.huffingtonpost.ca/news/' + search + '/?page='\n",
    "        else:\n",
    "            base_url_huff = 'https://www.huffpost.com/impact/topic/' + search + '?page='\n",
    "        \n",
    "        r_huff = Request(base_url_huff + page_number, headers={'User-Agent': 'Chrome/90.0.4430.212'})\n",
    "        webpage = urlopen(r_huff).read()\n",
    "        page_soup = soup(webpage,\"html.parser\")\n",
    "        page_class = page_soup.findAll(\"a\", {\"class\": \"card__headline card__headline--long\"})\n",
    "        \n",
    "        for link in page_class:\n",
    "            links.append([search, check_lst[idx], link.get('href')])\n",
    "        \n",
    "        time.sleep(5)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.DataFrame(links, columns=['Search', 'Check', 'Link'])\n",
    "df_links.to_csv('../Data/huff_links.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_huff = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(links)): \n",
    "    try:\n",
    "        article_huff = Article(links[i][2])\n",
    "        article_huff.download()\n",
    "        article_huff.parse()\n",
    "        article_huff.nlp()\n",
    "\n",
    "        article_info_huff = {'company': [links[i][0]],\n",
    "                             'link': [links[i][2]],\n",
    "                             'title' : [article_huff.title], \n",
    "                             'description': [article_huff.meta_description],\n",
    "                             'nlp_summary': [article_huff.summary],\n",
    "                             'publish date': [datetime.datetime.date(article_huff.publish_date)]}\n",
    "\n",
    "        temp_df = pd.DataFrame(article_info_huff)\n",
    "        df_huff = df_huff.append(temp_df)\n",
    "\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        article_info_huff = {'company': [links[i][0]],\n",
    "                             'link': [links[i][2]], \n",
    "                             'title': [\"null\"], \n",
    "                             'description': [\"null\"], \n",
    "                             'nlp_summary': [\"null\"],\n",
    "                             'publish date': [\"null\"]}\n",
    "\n",
    "        temp_df = pd.DataFrame(article_info_huff)\n",
    "        df_huff = df_huff.append(temp_df)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_huff.to_csv('../Data/huffpost_articles.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
